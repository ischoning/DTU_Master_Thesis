{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b3bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from struct import *\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import mixture\n",
    "sns.set_style(\"white\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import collections\n",
    "from math import pi, cos, sin, cosh, tanh\n",
    "from scipy.spatial.transform import Rotation as Rot\n",
    "import cv2\n",
    "import plotting\n",
    "from plotting import *\n",
    "import interpolation\n",
    "from interpolation import *\n",
    "import ficks\n",
    "from ficks import *\n",
    "import structure\n",
    "from structure import *\n",
    "import scipy.interpolate\n",
    "import scipy.integrate\n",
    "import scipy.stats\n",
    "from sklearn.linear_model import RANSACRegressor, TheilSenRegressor, LinearRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "from scipy.signal import argrelextrema\n",
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019054a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# DO NOT TOUCH. Define colors and states for plotting events.\n",
    "#\n",
    "STATES = ['fix', 'sac', 'smp', 'vor', 'blink', 'fix_blink', 'sac_blink', 'other', 'loss']\n",
    "COLORS = {'fix':'red', 'sac':'lime', 'smp':'purple', 'vor':'brown', 'blink':'dodgerblue', 'fix_blink':'darkorchid', 'sac_blink':'turquoise', 'other':'grey', 'loss':'gold'}\n",
    "BORDERS = {'fix':'magenta', 'sac':'green', 'smp':'blue', 'vor':'tan', 'blink':'royalblue', 'fix_blink':'darkviolet', 'sac_blink':'lightseagreen', 'other':'black', 'loss':'yellow'}\n",
    "BINS = 50\n",
    "\n",
    "# All participants and their respective vision loss\n",
    "vision_loss = {'P2':'CVL', 'P3':'CVL', 'P4':'Other', 'P5':'PVL', 'P6':'CVL', 'P8':'CVL', 'P9':'CVL', 'P10':'CVL',\n",
    "                'P11':'PVL', 'P12':'PVL', 'P13':'CVL', 'P14':'PVL', 'P15':'PVL', 'P16':'PVL', 'P17':'PVL',\n",
    "                'P18':'CVL', 'P19':'CVL', 'P21':'CVL', 'P22':'PVL', 'P23':'PVL', 'P24':'PVL', 'P25':'CVL'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c641b3",
   "metadata": {},
   "source": [
    "### probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c84c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, std):\n",
    "    return (1/(std*np.sqrt(2*np.pi)))*np.exp(-0.5*((x-mu)/std)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197e17f",
   "metadata": {},
   "source": [
    "### functions used for Carpenter's theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1846d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carpenter(a):\n",
    "    \"\"\"\n",
    "    A function that takes in amplitude (deg) and returns duration (sec).\n",
    "    \"\"\"\n",
    "    return (21 + 2.2*a)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ac0757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in_path = '/data/Isabella/thesis_spring2022/event_detect_in/'\n",
    "\n",
    "# for fname in os.listdir(in_path):\n",
    "#     ID = fname.partition('.csv')[0] \n",
    "#     if 'hand_wash' in ID:\n",
    "#         DEVICE, _, __, eye, subID = ID.split('_')\n",
    "#         task = 'handwash'\n",
    "#         os.rename(in_path+fname, in_path+f'{DEVICE}_{task}_{eye}_{subID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69cd06",
   "metadata": {},
   "source": [
    "## Steps to event detection:\n",
    "\n",
    "### Step 0: Pre-process\n",
    "- 0.1 Calculate intersample velocity v.\n",
    "- 0.2 Remove noise by replacing x,y,v with NAN where v > 1000 and v<=0.\n",
    "- 0.3 Label loss where there is a difference in time greater than 400 ms between samples.\n",
    "- 0.4 Up-sample angular positional features x, y where there are gaps in data above 3xSAMPLE_RATE but less than 400 ms using cubic intervariate spline method.\n",
    "- 0.5 Interpolate the upsampled angular positional features (Dx, Dy) and find their derivatives.\n",
    "\n",
    "### Step 1: Low- vs High-speed events\n",
    "- Calculate feature Dn = sqrt(diff(dDx/dt)+diff(dDy/dt))\n",
    "- Run EM-GMM to calculate priors and likelihoods for low- and high-speed.\n",
    "- Calculate probabilities for low- and high-speed sample-by-sample.\n",
    "- Label samples where p(L) >= p(H) as low-speed, otherwise high-speed. \n",
    "- Create events_df from labelled events.\n",
    "\n",
    "### Step 2: Within movement classify saccades and blinks\n",
    "- Calculate feature Dv = abs(max-min) from diff(dDy/dt) (vertical interpolated velocity) for all non-loss events.\n",
    "- Run EM-GMM on distribution of Dv in high-speed events to calculate priors and likelihoods for sac or blink.\n",
    "- Calculate joint probabilities for sac and blink event-by-event, conditional on the probability of the event being a high-speed event.\n",
    "\n",
    "### Step 3: Within fixations distinguish between fixations and smooth pursuits\n",
    "(which could also be VOR since we don't account for head rotation)\n",
    "- Calculate amplitude feature as Da = sqrt((Dx_t-Dx_0)^2+(Dy_t-Dy_0)^2) for all non-loss events (unless fixation, then Da = std(x)+std(y).\n",
    "- Run EM-GMM on distribution of Da in low-speed events to calculate priors and likelihoods for fix or smp.\n",
    "- Calculate probabilities for fix or smp event-by-event, conditional on the probability of the event being a low-speed event.\n",
    "\n",
    "### Step 4: Classify events\n",
    "- For each event in events_df, classify the event as that which has the highest probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee35f9",
   "metadata": {},
   "source": [
    "## Run event detection, one dataset at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603b2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "varjo_to_use = {'P2':['cereal'], 'P3':['cereal', 'sandwich'], 'P5':['cereal', 'sandwich'], \n",
    "                'P6':['cereal', 'sandwich'], 'P8':['sandwich'], 'P9':['cereal', 'sandwich'],\n",
    "                'P13':['cereal', 'sandwich'], 'P14':['cereal', 'sandwich'], 'P15':['cereal'],\n",
    "                'P18':['cereal', 'sandwich'], 'P19':['cereal', 'sandwich'],\n",
    "                'P21':['cereal'], 'P22':['cereal', 'sandwich'], 'P23':['cereal', 'sandwich'],\n",
    "                'P24':['cereal', 'sandwich'], 'P25':['cereal', 'sandwich']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc50b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "varjo_cereal_right_P19 processing...\n",
      "[1] varjo_cereal_right_P19 success!\n",
      "varjo_cereal_left_P19 processing...\n",
      "[2] varjo_cereal_left_P19 success!\n",
      "varjo_sandwich_left_P16 processing...\n",
      "[3] varjo_sandwich_left_P16 success!\n",
      "varjo_sandwich_left_P24 processing...\n",
      "[4] varjo_sandwich_left_P24 success!\n",
      "varjo_cereal_left_P24 processing...\n",
      "[5] varjo_cereal_left_P24 success!\n",
      "varjo_cereal_left_P6 processing...\n",
      "[6] varjo_cereal_left_P6 success!\n",
      "varjo_cereal_left_P5 processing...\n",
      "[7] varjo_cereal_left_P5 success!\n",
      "varjo_cereal_right_P24 processing...\n",
      "[8] varjo_cereal_right_P24 success!\n",
      "varjo_cereal_right_P21 processing...\n",
      "[9] varjo_cereal_right_P21 success!\n",
      "varjo_sandwich_right_P16 processing...\n",
      "[10] varjo_sandwich_right_P16 success!\n",
      "varjo_cereal_left_P9 processing...\n",
      "[11] varjo_cereal_left_P9 success!\n",
      "varjo_cereal_left_P14 processing...\n",
      "[12] varjo_cereal_left_P14 success!\n",
      "varjo_cereal_left_P16 processing...\n",
      "[13] varjo_cereal_left_P16 success!\n",
      "varjo_cereal_right_P5 processing...\n",
      "[14] varjo_cereal_right_P5 success!\n",
      "varjo_cereal_left_P15 processing...\n",
      "[15] varjo_cereal_left_P15 success!\n",
      "varjo_cereal_left_P13 processing...\n",
      "[16] varjo_cereal_left_P13 success!\n",
      "varjo_sandwich_left_P18 processing...\n",
      "[17] varjo_sandwich_left_P18 success!\n",
      "varjo_sandwich_left_P9 processing...\n",
      "[18] varjo_sandwich_left_P9 success!\n",
      "varjo_sandwich_right_P25 processing...\n",
      "[19] varjo_sandwich_right_P25 success!\n",
      "varjo_cereal_right_P22 processing...\n",
      "[20] varjo_cereal_right_P22 success!\n",
      "varjo_sandwich_right_P23 processing...\n",
      "[21] varjo_sandwich_right_P23 success!\n",
      "varjo_cereal_right_P23 processing...\n",
      "[22] varjo_cereal_right_P23 success!\n",
      "varjo_sandwich_right_P22 processing...\n",
      "[23] varjo_sandwich_right_P22 success!\n",
      "varjo_sandwich_left_P25 processing...\n",
      "[24] varjo_sandwich_left_P25 success!\n",
      "varjo_cereal_left_P22 processing...\n",
      "[25] varjo_cereal_left_P22 success!\n",
      "varjo_sandwich_right_P13 processing...\n",
      "[26] varjo_sandwich_right_P13 success!\n",
      "varjo_sandwich_right_P6 processing...\n",
      "[27] varjo_sandwich_right_P6 success!\n",
      "varjo_sandwich_right_P14 processing...\n",
      "[28] varjo_sandwich_right_P14 success!\n",
      "varjo_cereal_right_P18 processing...\n",
      "[29] varjo_cereal_right_P18 success!\n",
      "varjo_sandwich_right_P18 processing...\n",
      "[30] varjo_sandwich_right_P18 success!\n",
      "varjo_sandwich_right_P5 processing...\n",
      "[31] varjo_sandwich_right_P5 success!\n",
      "varjo_sandwich_left_P23 processing...\n",
      "[32] varjo_sandwich_left_P23 success!\n",
      "varjo_sandwich_right_P3 processing...\n",
      "[33] varjo_sandwich_right_P3 success!\n",
      "varjo_cereal_right_P16 processing...\n",
      "[34] varjo_cereal_right_P16 success!\n",
      "varjo_cereal_right_P2 processing...\n",
      "[35] varjo_cereal_right_P2 success!\n",
      "varjo_cereal_left_P25 processing...\n",
      "[36] varjo_cereal_left_P25 success!\n",
      "varjo_cereal_left_P2 processing...\n",
      "[37] varjo_cereal_left_P2 success!\n",
      "varjo_cereal_right_P15 processing...\n",
      "[38] varjo_cereal_right_P15 success!\n",
      "varjo_sandwich_left_P6 processing...\n",
      "[39] varjo_sandwich_left_P6 success!\n",
      "varjo_cereal_right_P6 processing...\n",
      "[40] varjo_cereal_right_P6 success!\n",
      "varjo_cereal_right_P3 processing...\n",
      "[41] varjo_cereal_right_P3 success!\n",
      "varjo_cereal_left_P18 processing...\n",
      "[42] varjo_cereal_left_P18 success!\n",
      "varjo_sandwich_left_P8 processing...\n",
      "[43] varjo_sandwich_left_P8 success!\n",
      "varjo_cereal_right_P14 processing...\n",
      "[44] varjo_cereal_right_P14 success!\n",
      "varjo_sandwich_right_P9 processing...\n",
      "[45] varjo_sandwich_right_P9 success!\n",
      "varjo_cereal_left_P23 processing...\n",
      "[46] varjo_cereal_left_P23 success!\n",
      "varjo_cereal_left_P3 processing...\n",
      "[47] varjo_cereal_left_P3 success!\n",
      "varjo_sandwich_left_P13 processing...\n",
      "[48] varjo_sandwich_left_P13 success!\n",
      "varjo_sandwich_left_P14 processing...\n",
      "[49] varjo_sandwich_left_P14 success!\n",
      "varjo_sandwich_left_P22 processing...\n",
      "[50] varjo_sandwich_left_P22 success!\n",
      "varjo_cereal_right_P25 processing...\n",
      "[51] varjo_cereal_right_P25 success!\n",
      "varjo_cereal_right_P9 processing...\n",
      "[52] varjo_cereal_right_P9 success!\n",
      "varjo_sandwich_right_P8 processing...\n",
      "[53] varjo_sandwich_right_P8 success!\n",
      "varjo_sandwich_right_P19 processing...\n",
      "[54] varjo_sandwich_right_P19 success!\n",
      "varjo_sandwich_left_P19 processing...\n",
      "[55] varjo_sandwich_left_P19 success!\n",
      "varjo_cereal_left_P21 processing...\n",
      "[56] varjo_cereal_left_P21 success!\n",
      "varjo_sandwich_left_P3 processing...\n",
      "[57] varjo_sandwich_left_P3 success!\n",
      "varjo_sandwich_right_P24 processing...\n",
      "[58] varjo_sandwich_right_P24 success!\n",
      "varjo_cereal_right_P13 processing...\n",
      "varjo_cereal_right_P13 FAILED - noisy data below Nyquist threshold\n",
      "varjo_sandwich_left_P5 processing...\n",
      "[59] varjo_sandwich_left_P5 success!\n"
     ]
    }
   ],
   "source": [
    "sample_rates = {'varjo':200, 'PI':66}\n",
    "in_path = '/data/Isabella/thesis_spring2022/event_detect_in/'\n",
    "out_path = '/data/Isabella/thesis_spring2022/event_detect_out_final/'\n",
    "\n",
    "NUM = 1\n",
    "for fname in os.listdir(in_path):\n",
    "    \n",
    "    ID = fname.partition('.csv')[0]\n",
    "    DEVICE, task, eye, subID = ID.split('_')\n",
    "    SAMPLE_RATE = sample_rates[DEVICE]\n",
    "    MIN_FIX_DUR = 0.1\n",
    "    \n",
    "    # only run on varjo\n",
    "    if DEVICE == 'PI':\n",
    "        continue\n",
    "    \n",
    "    # do only varjo used in NN\n",
    "    if subID in varjo_to_use.keys():\n",
    "        if task not in varjo_to_use[subID]:\n",
    "            continue\n",
    "    else: continue\n",
    "        \n",
    "    # only run on datasets that have not been run\n",
    "    if fname in os.listdir(out_path):\n",
    "        print(f'[{NUM}] {ID} already exists in {out_path}')\n",
    "        NUM += 1\n",
    "        continue\n",
    "    \n",
    "#     # Initialize output dataframe for each combination of HMD:task:eye:subID\n",
    "#     sequences = pd.DataFrame({})\n",
    "#     test_seq = sequences.copy()\n",
    "#     test_seq_51 = sequences.copy()\n",
    "#     test_seq_52 = sequences.copy()\n",
    "    \n",
    "    print(f'{ID} processing...')\n",
    "\n",
    "    df = pd.read_csv(in_path+fname)\n",
    "    df['event'] = 'other'\n",
    "    \n",
    "    # Detect loss and replace with NANs\n",
    "    max_interp_dur = 0.4 # seconds\n",
    "    to_drop = []\n",
    "    for idx, row in df[1:].copy().iterrows():\n",
    "        if idx in to_drop:\n",
    "            continue\n",
    "        i = idx+1\n",
    "        prev_row = df.loc[idx-1]\n",
    "        dt = row.time - prev_row.time\n",
    "        if dt > max_interp_dur:\n",
    "            df.loc[idx-1:i, 'event'] = 'loss'\n",
    "            df.loc[idx-1:i, 'x_deg':'issy'] = np.NAN\n",
    "\n",
    "    df['x_deg'] = df['x_deg'].astype(np.float)\n",
    "    df['y_deg'] = df['y_deg'].astype(np.float)\n",
    "    \n",
    "    # Calculate interpolated velocity\n",
    "    interp_v = interpolate(df, SAMPLE_RATE, feat='iss')\n",
    "    df['Dn'] = interp_v(df.time)\n",
    "    \n",
    "    #\n",
    "    # Step 1. Low-vs-High-speed events\n",
    "    #\n",
    "    # set features\n",
    "    Dn = 'Dn' # interpolated iss (quadratic)\n",
    "\n",
    "    # estimate model parameters with the EM algorithm\n",
    "    Dn_mixture = GaussianMixture(n_components=2).fit(df.loc[df.event!='loss', Dn].to_numpy().reshape(-1,1))\n",
    "    Dn_means_hat = Dn_mixture.means_.flatten()\n",
    "    Dn_weights_hat = Dn_mixture.weights_.flatten()\n",
    "    Dn_sds_hat = np.sqrt(Dn_mixture.covariances_).flatten()\n",
    "\n",
    "    # fix should have smaller mean than non-fix\n",
    "    fixi = np.argmin(Dn_means_hat)\n",
    "    nfixi = np.argmax(Dn_means_hat)\n",
    "    Dn_mu1, Dn_sd1 = Dn_means_hat[fixi], Dn_sds_hat[fixi]\n",
    "    Dn_mu2, Dn_sd2 = Dn_means_hat[nfixi], Dn_sds_hat[nfixi]\n",
    "\n",
    "    prior_fix = Dn_weights_hat[fixi]\n",
    "    prior_nonfix = Dn_weights_hat[nfixi]\n",
    "    \n",
    "    # sklearn's gaussian mixture model uses EM algorithm to estimate model parameters\n",
    "    df['P_fix'] = 0.0\n",
    "    df['P_nonfix'] = 0.0\n",
    "    df['L_fix'] = 0.0\n",
    "    df['L_nonfix'] = 0.0\n",
    "    for idx, row in df.iterrows():\n",
    "        if row[Dn] < Dn_mu1:\n",
    "            L_fix = norm.pdf(Dn_mu1, Dn_mu1, Dn_sd1)\n",
    "        else:\n",
    "            L_fix = norm.pdf(row[Dn], Dn_mu1, Dn_sd1)\n",
    "        if row[Dn] > Dn_mu2:\n",
    "            L_nonfix = norm.pdf(Dn_mu2, Dn_mu2, Dn_sd2)\n",
    "        else:\n",
    "            L_nonfix = norm.pdf(row[Dn], Dn_mu2, Dn_sd2)\n",
    "        df.loc[idx, 'L_fix'] = L_fix\n",
    "        df.loc[idx, 'L_nonfix'] = L_nonfix\n",
    "        df.loc[idx, 'P_fix'] = (L_fix*prior_fix)/(L_fix*prior_fix + L_nonfix*prior_nonfix)\n",
    "        df.loc[idx, 'P_nonfix'] = (L_nonfix*prior_nonfix)/(L_fix*prior_fix + L_nonfix*prior_nonfix)\n",
    "    \n",
    "    # Classify fixations as those with probability greater than or equal to 50%.\n",
    "    df['event'] = np.where(df.event!='loss', np.where(df.P_fix>=0.5, 'fix', df.event), df.event)\n",
    "\n",
    "    # Make events_df (need to iterate twice)\n",
    "    # Ignore fixation events where the duration is less than the minimum duration of a fixation.\n",
    "    events_df = make_events_df(df, interp_v, SAMPLE_RATE, MIN_FIX_DUR)\n",
    "    # running it again means that consecutive events are merged into one\n",
    "    df = map_to_stream(events_df, df)\n",
    "    events_df = make_events_df(df, interp_v, SAMPLE_RATE, MIN_FIX_DUR)\n",
    "    \n",
    "    # mark events with calculus error above 1.5 as noise (non-movements)\n",
    "    events_df.loc[events_df.calculus_error>1.5, 'event'] = 'noise'\n",
    "    df = map_to_stream(events_df, df)\n",
    "    \n",
    "\n",
    "    #\n",
    "    # Step 2: Sac vs Blink\n",
    "    #\n",
    "    # set Dv feature\n",
    "    Dv = 'calculus_error'\n",
    "\n",
    "    # estimate model parameters with the EM algorithm for non-fix events\n",
    "    #  if an error is found here, then that is because the model does not have enough high-speed events\n",
    "    #  and has instead found a lot of noise in the dataset so it should not be processes\n",
    "    #  (it means consecutive non-nan events are not above Nyquist threshold)\n",
    "    try:\n",
    "        Dv_mixture = GaussianMixture(n_components=2).fit(events_df.loc[events_df.event=='other', Dv].to_numpy().reshape(-1,1))\n",
    "    except:\n",
    "        print(f'{ID} FAILED - noisy data below Nyquist threshold')\n",
    "        continue\n",
    "    Dv_means_hat = Dv_mixture.means_.flatten()\n",
    "    Dv_weights_hat = Dv_mixture.weights_.flatten()\n",
    "    Dv_sds_hat = np.sqrt(Dv_mixture.covariances_).flatten()\n",
    "\n",
    "    # sac should have smaller mean than blink\n",
    "    saci = np.argmin(Dv_means_hat)\n",
    "    blinki = np.argmax(Dv_means_hat)\n",
    "    Dv_mu1, Dv_sd1 = Dv_means_hat[saci], Dv_sds_hat[saci]\n",
    "    Dv_mu2, Dv_sd2 = Dv_means_hat[blinki], Dv_sds_hat[blinki]\n",
    "\n",
    "    prior_sac = Dv_weights_hat[saci]\n",
    "    prior_blink = Dv_weights_hat[blinki]\n",
    "    \n",
    "    # sklearn's gaussian mixture model uses EM algorithm to estimate model parameters\n",
    "    events_df['P_sac'] = 0.0\n",
    "    events_df['P_blink'] = 0.0\n",
    "    events_df['L_sac'] = 0.0\n",
    "    events_df['L_blink'] = 0.0\n",
    "    for idx, row in events_df.loc[events_df.event!='noise'].iterrows():\n",
    "        if row[Dv] <= Dv_mu1:\n",
    "            L_sac = norm.pdf(Dv_mu1, Dv_mu1, Dv_sd1)\n",
    "        else:\n",
    "            L_sac = norm.pdf(row[Dv], Dv_mu1, Dv_sd1)\n",
    "        if row[Dv] > Dv_mu2:\n",
    "            L_blink = norm.pdf(Dv_mu2, Dv_mu2, Dv_sd2)\n",
    "        else:\n",
    "            L_blink = norm.pdf(row[Dv], Dv_mu2, Dv_sd2)\n",
    "        events_df.loc[idx, 'L_sac'] = L_sac\n",
    "        events_df.loc[idx, 'L_blink'] = L_blink\n",
    "        events_df.loc[idx, 'P_sac'] = ((L_sac*prior_sac)/(L_sac*prior_sac + L_blink*prior_blink))*row.P_nonfix\n",
    "        events_df.loc[idx, 'P_blink'] = ((L_blink*prior_blink)/(L_sac*prior_sac + L_blink*prior_blink))*row.P_nonfix\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Step 3: Fix vs Smp\n",
    "    #\n",
    "    # set Da feature\n",
    "    Da = 'dispersion'\n",
    "\n",
    "    # estimate model parameters with the EM algorithm\n",
    "    Da_mixture = GaussianMixture(n_components=2).fit(events_df.loc[events_df.event=='fix', Da].to_numpy().reshape(-1,1))\n",
    "    Da_means_hat = Da_mixture.means_.flatten()\n",
    "    Da_weights_hat = Da_mixture.weights_.flatten()\n",
    "    Da_sds_hat = np.sqrt(Da_mixture.covariances_).flatten()\n",
    "\n",
    "    # fix should have smaller mean than smp/vor\n",
    "    fixi = np.argmin(Da_means_hat)\n",
    "    smpi = np.argmax(Da_means_hat)\n",
    "    Da_mu1, Da_sd1 = Da_means_hat[fixi], Da_sds_hat[fixi]\n",
    "    Da_mu2, Da_sd2 = Da_means_hat[smpi], Da_sds_hat[smpi]\n",
    "\n",
    "    prior_ff = Da_weights_hat[fixi]\n",
    "    prior_smp = Da_weights_hat[smpi]\n",
    "    \n",
    "    # sklearn's gaussian mixture model uses EM algorithm to estimate model parameters\n",
    "    events_df['P_ff'] = 0.0\n",
    "    events_df['P_smp'] = 0.0\n",
    "    events_df['L_ff'] = 0.0\n",
    "    events_df['L_smp'] = 0.0\n",
    "    for idx, row in events_df.iterrows():\n",
    "        if row[Da] <= Da_mu1:\n",
    "            L_ff = norm.pdf(Da_mu1, Da_mu1, Da_sd1)\n",
    "        else:\n",
    "            L_ff = norm.pdf(row[Da], Da_mu1, Da_sd1)\n",
    "        if row[Da] > Da_mu2:\n",
    "            L_smp = norm.pdf(Da_mu2, Da_mu2, Da_sd2)\n",
    "        else:\n",
    "            L_smp = norm.pdf(row[Da], Da_mu2, Da_sd2)\n",
    "        events_df.loc[idx, 'L_ff'] = L_ff\n",
    "        events_df.loc[idx, 'L_smp'] = L_smp\n",
    "        events_df.loc[idx, 'P_ff'] = (L_ff*prior_ff)/(L_ff*prior_ff + L_smp*prior_smp)*row.P_fix\n",
    "        events_df.loc[idx, 'P_smp'] = (L_smp*prior_smp)/(L_ff*prior_ff + L_smp*prior_smp)*row.P_fix\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Step 4: Classify events and evaluate error metrics\n",
    "    #\n",
    "    # Classify events as that with the largest probability\n",
    "    s = ['fix', 'smp', 'sac', 'blink']\n",
    "    for idx, row in events_df[(events_df.event!='loss')&(events_df.event!='noise')].iterrows():\n",
    "        i = np.argmax([row.P_ff, row.P_smp, row.P_sac, row.P_blink])\n",
    "        events_df.loc[idx, 'event'] = s[i]\n",
    "        \n",
    "    # map the new events_df to df\n",
    "    df = map_to_stream(events_df, df)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Step 4.1: Adjustments\n",
    "    #\n",
    "    # Merge events broken up by noise with recalculated features and drop the noise\n",
    "    to_drop = []\n",
    "    for idx, row in events_df.copy().iterrows():\n",
    "        if idx == 0 or idx == len(events_df)-1:\n",
    "            continue\n",
    "\n",
    "        prev_row = events_df.loc[idx-1]\n",
    "        next_row = events_df.loc[idx+1]\n",
    "\n",
    "        # if noise occurs between the same event\n",
    "        if row.event == 'noise' and prev_row.event == next_row.event:\n",
    "\n",
    "            events_df = merge_events(df, events_df, idx-1, idx+1, prev_row.event, interp_v, SAMPLE_RATE)\n",
    "            to_drop.extend([idx,idx+1])\n",
    "\n",
    "    # drop events that are now merged\n",
    "    events_df = events_df.drop(to_drop)\n",
    "    events_df.reset_index(inplace=True)\n",
    "    del events_df['index']\n",
    "\n",
    "    # map the updated events_df to df\n",
    "    df = map_to_stream(events_df, df)\n",
    "\n",
    "    # adjust amplitude and related features for non-fixations between two fixations\n",
    "    events_df = adjust_amp_dur(events_df, df, interp_v)\n",
    "    \n",
    "\n",
    "    # ADJUSTMENTS\n",
    "    # relabel smp as sac if carpenter error is less than threshold\n",
    "    avg_sac_carp_error_plus_two_std = events_df[events_df.event=='sac'].carpenter_error.mean()+2*events_df[events_df.event=='sac'].carpenter_error.std()\n",
    "    events_df.loc[(events_df.event=='smp')&(events_df.carpenter_error<=avg_sac_carp_error_plus_two_std), 'event'] = 'sac'\n",
    "\n",
    "    # remove saccades and smooth pursuits with amplitude below 1.5\n",
    "    events_df = events_df.drop(events_df.loc[(events_df.amplitude<1.5)&(events_df.event=='sac')].index)\n",
    "    events_df.reset_index(inplace=True)\n",
    "    del events_df['index']\n",
    "    events_df = events_df.drop(events_df.loc[(events_df.amplitude<1.5)&(events_df.event=='smp')].index)\n",
    "    events_df.reset_index(inplace=True)\n",
    "    del events_df['index']\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Algo 5.1: Merge around blinks\n",
    "    #\n",
    "    test_df = df.copy()\n",
    "    test_events_df = events_df.copy()\n",
    "    \n",
    "    # Calculate statistical thresholds based on the data:\n",
    "    # use two std for amplitude bc system noise in spatial data\n",
    "    # use one std for carpenter bc error calculation\n",
    "    avg_fix_amp_plus_two_std = test_events_df[test_events_df.event=='fix'].amplitude.mean()+2*test_events_df[test_events_df.event=='fix'].amplitude.std()\n",
    "    avg_sac_carp_error_plus_two_std = test_events_df[test_events_df.event=='sac'].carpenter_error.mean()+2*test_events_df[test_events_df.event=='sac'].carpenter_error.std()\n",
    "    avg_sac_calc_error_plus_two_std = test_events_df[test_events_df.event=='sac'].calculus_error.mean()+test_events_df[test_events_df.event=='sac'].calculus_error.std()\n",
    "\n",
    "    to_drop = []\n",
    "    for idx, row in test_events_df.copy().iterrows():\n",
    "        if idx == 0 or idx == len(test_events_df)-1:\n",
    "            continue\n",
    "\n",
    "        prev_row = test_events_df.loc[idx-1]\n",
    "        next_row = test_events_df.loc[idx+1]\n",
    "\n",
    "        # if a blink or noise occurs between the same event\n",
    "        if row.event == 'blink' and prev_row.event == next_row.event:\n",
    "\n",
    "            # if the neighboring events are fixations whose centers vary less than a threshold,\n",
    "            #  then merge the two neighboring events\n",
    "            if prev_row.event == 'fix' and np.sqrt((next_row.center_x-prev_row.center_x)**2+(next_row.center_y-prev_row.center_y)**2) <= avg_fix_amp_plus_two_std:\n",
    "                test_events_df = merge_events(test_df, test_events_df,idx-1,idx+1,'fix',interp_v, SAMPLE_RATE,blink=True)\n",
    "                to_drop.extend([idx,idx+1])\n",
    "                # Flag in df where the blink used to be\n",
    "                test_df.loc[row.start_i:row.end_i, 'has_blink'] = True\n",
    "            else:   \n",
    "                amplitude = np.sqrt((next_row.x0-prev_row.xn)**2+(next_row.y0-prev_row.yn)**2)\n",
    "                duration = next_row.start_s-prev_row.end_s\n",
    "                carpenter_error = np.abs(duration*1000 - (21 + 2.2 * amplitude))/(21 + 2.2 * amplitude)\n",
    "                # if the blink follows saccadic behavior according to carpenter, \n",
    "                # then it's likely a saccade with blink\n",
    "                if carpenter_error <= avg_sac_carp_error_plus_two_std:\n",
    "                    # if the neighboring events are also saccades, then merge into one saccade\n",
    "                    if prev_row.event == 'sac':\n",
    "                        test_events_df = merge_events(test_df, test_events_df,idx-1,idx+1, 'sac', interp_v, SAMPLE_RATE,blink=True)\n",
    "                        to_drop.extend([idx,idx+1])\n",
    "                    # otherwise reclassify the blink as a saccade\n",
    "                    else:\n",
    "                        test_df.loc[row.start_i:row.end_i, 'event'] = 'sac'\n",
    "                        test_events_df.loc[idx, 'event'] = 'sac'\n",
    "                        test_events_df.loc[idx, 'has_blink'] = True\n",
    "                        # Flag in df where the blink used to be\n",
    "                        test_df.loc[row.start_i:row.end_i, 'has_blink'] = True\n",
    "                # otherwise merge the neigboring events, ignoring the blink\n",
    "                else:\n",
    "                    event = prev_row.event\n",
    "                    test_events_df = merge_events(test_df, test_events_df,idx-1,idx+1,event,interp_v, SAMPLE_RATE,blink=True)\n",
    "                    to_drop.extend([idx,idx+1])\n",
    "                    # Flag in df where the blink used to be\n",
    "                    test_df.loc[row.start_i:row.end_i, 'has_blink'] = True\n",
    "\n",
    "        # Otherwise, if a blink breaks up separate events (neither of which is loss)\n",
    "        elif row.event == 'blink' and prev_row.event!='loss' and next_row.event!='loss':\n",
    "            amplitude = np.sqrt((next_row.x0-prev_row.xn)**2+(next_row.y0-prev_row.yn)**2)\n",
    "            duration = next_row.start_s-prev_row.end_s\n",
    "            carpenter_error = np.abs(duration*1000 - (21 + 2.2 * amplitude))/(21 + 2.2 * amplitude)\n",
    "            # if the blink follows saccadic behavior according to carpenter, \n",
    "            # then it's likely a saccade with blink\n",
    "            if carpenter_error <= avg_sac_carp_error_plus_two_std:\n",
    "                test_df.loc[row.start_i:row.end_i, 'event'] = 'sac'\n",
    "                test_events_df.loc[idx, 'event'] = 'sac'\n",
    "                test_df.loc[row.start_i:row.end_i, 'has_blink'] = True\n",
    "                test_events_df.loc[idx, 'has_blink'] = True\n",
    "                # if one of the neighboring events is also a saccade, then merge them together\n",
    "                if prev_row.event == 'sac':\n",
    "                    test_events_df = merge_events(test_df, test_events_df,idx-1,idx,'sac',interp_v, SAMPLE_RATE,blink=True)\n",
    "                    # Drop the event\n",
    "                    to_drop.extend([idx])\n",
    "                elif next_row.event == 'sac':\n",
    "                    test_events_df = merge_events(test_df, test_events_df,idx,idx+1,'sac',interp_v, SAMPLE_RATE,blink=True)\n",
    "                    # Drop the next event\n",
    "                    to_drop.extend([idx+1])\n",
    "\n",
    "            # otherwise, leave as blink\n",
    "            else:\n",
    "                test_df.loc[row.start_i:row.end_i, 'has_blink'] = True\n",
    "                test_events_df.loc[idx, 'has_blink'] = True\n",
    "\n",
    "    # drop events that are now merged\n",
    "    test_events_df = test_events_df.drop(to_drop)\n",
    "    test_events_df.reset_index(inplace=True)\n",
    "    del test_events_df['index']\n",
    "\n",
    "    # map the new events_df to df for plotting purposes\n",
    "    test_df = map_to_stream(events_df, df)\n",
    "\n",
    "    # adjust amplitude and related featurese\n",
    "    test_events_df = adjust_amp_dur(test_events_df, test_df, interp_v)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Algo 5.2: Find missing saccades between consecutive events (otherwise merge)\n",
    "    #\n",
    "    test_df_51 = test_df.copy()\n",
    "    test_events_df_51 = test_events_df.copy()\n",
    "\n",
    "    # find all consecutive events and add a saccade between them if there is a spatial jump\n",
    "    # greater than threshold (otherwise merge)\n",
    "    to_add, values = [], []\n",
    "    test_events_df['drop'] = 0\n",
    "    for idx, row in test_events_df.iterrows():\n",
    "        if idx == 0:\n",
    "            continue\n",
    "        prev_row = test_events_df.loc[idx-1]\n",
    "        if row.event == prev_row.event:\n",
    "            x0, xn = test_df.loc[prev_row.end_i,'x_deg'], test_df.loc[row.start_i,'x_deg']\n",
    "            y0, yn = test_df.loc[prev_row.end_i,'y_deg'], test_df.loc[row.start_i,'y_deg']\n",
    "            t0, tn = test_df.loc[prev_row.end_i,'time'], test_df.loc[row.start_i,'time']\n",
    "            if row.event == 'fix':\n",
    "                # calculate distance between centers of consecutive events\n",
    "                A = np.sqrt((row.center_x-prev_row.center_x)**2+(row.center_y-prev_row.center_y)**2)\n",
    "            else:\n",
    "                # calculate distance between last and first of samples of consecutive events\n",
    "                A = np.sqrt((xn-x0)**2+(yn-y0)**2)\n",
    "            # add a saccade there if it's greater than the fixation amplitude threshold\n",
    "            if A > avg_fix_amp_plus_two_std:\n",
    "                to_add.append(idx)\n",
    "                D = tn - t0\n",
    "                max_vel, avg_vel, std_vel = interpolate_velocity([t0, tn], interp_v, SAMPLE_RATE)\n",
    "                avg_Dn = np.nanmean([prev_row.avg_Dn, row.avg_Dn])\n",
    "                avg_iss = A/D\n",
    "                carpenter_error = np.abs((D*1000 - (21 + 2.2 * A))/(21 + 2.2 * A))\n",
    "                integral = interp_v.integral(t0, tn)\n",
    "                calculus_error = np.abs((A-integral)/integral)\n",
    "                values.append(['sac', #event\n",
    "                               prev_row.end_i, #start_i\n",
    "                               row.start_i, #end_i\n",
    "                               t0,    #start_s\n",
    "                               tn,    #end_s\n",
    "                               D,   #duration\n",
    "                               A,   #amplitude\n",
    "                               np.nan,  #dispersion\n",
    "                               np.nan, #center_x\n",
    "                               np.nan, #center_y\n",
    "                               x0, #x0\n",
    "                               y0, #y0\n",
    "                               xn, #xn\n",
    "                               yn, #yn\n",
    "                               np.nan, #std_x\n",
    "                               np.nan, #std_y\n",
    "                               np.nan, #cov_x\n",
    "                               np.nan, #cov_y\n",
    "                               np.nan, #cov_xy\n",
    "                               np.nan, #max_Dn\n",
    "                               np.nan, #avg_Dn\n",
    "                               np.nan, #max_iss\n",
    "                               avg_iss, #avg_iss\n",
    "                               max_vel, #max_vel\n",
    "                               avg_vel, #avg_vel\n",
    "                               std_vel, #std_vel\n",
    "                               np.nan, #P_fix\n",
    "                               np.nan, #P_nonfix\n",
    "                               np.nan, #max_P_nonfix\n",
    "                               np.nan, #L_fix\n",
    "                               np.nan, #L_nonfix\n",
    "                               np.nan, #P_ff\n",
    "                               np.nan, #P_smp\n",
    "                               np.nan, #P_sac\n",
    "                               np.nan, #P_blink\n",
    "                               np.nan, #L_ff\n",
    "                               np.nan, #L_smp\n",
    "                               np.nan, #L_sac\n",
    "                               np.nan, #L_blink\n",
    "                               calculus_error, #calculus_error\n",
    "                               carpenter_error, #carpenter_error\n",
    "                               0, #has_blink\n",
    "                               0]) #drop\n",
    "\n",
    "            # otherwise merge the events\n",
    "            else:\n",
    "                # if the previous event has already been merged, then merge current event with the\n",
    "                # first event in the consecutive sequence\n",
    "                i = idx\n",
    "                while i > 0 and test_events_df.loc[i-1, 'drop'] == 1 and row.event == test_events_df.loc[i-1, 'event']:\n",
    "                    i -= 1\n",
    "                event = test_events_df.loc[idx, 'event']\n",
    "                test_events_df = merge_events(test_df, test_events_df,i-1,idx,event,interp_v, SAMPLE_RATE)\n",
    "                test_events_df.loc[idx, 'drop'] = 1\n",
    "\n",
    "    # add the additional saccades to the data, if any\n",
    "    if len(to_add) > 0:\n",
    "        test_events_df = pd.DataFrame(np.insert(test_events_df.values, to_add, values, axis=0))\n",
    "        test_events_df.columns = events_df.columns.tolist()+['drop']\n",
    "\n",
    "    # remove any rows that have been flagged as drop\n",
    "    to_drop = test_events_df[test_events_df['drop']==1].index.tolist()\n",
    "\n",
    "    if len(to_drop) > 0:\n",
    "        # drop events that are now merged\n",
    "        test_events_df = test_events_df.drop(to_drop)\n",
    "        test_events_df.reset_index(inplace=True)\n",
    "        del test_events_df['index']\n",
    "\n",
    "        # map the new events_df to df for plotting purposes\n",
    "        test_df = map_to_stream(events_df, df)\n",
    "\n",
    "        # adjust endpoints, amplitude, duration\n",
    "        test_events_df = adjust_amp_dur(test_events_df, df, interp_v)\n",
    "    \n",
    "    \n",
    "    #\n",
    "    # Step 5.3: Add final physiological checks\n",
    "    #\n",
    "    test_df_52 = test_df.copy()\n",
    "    test_events_df_52 = test_events_df.copy()\n",
    "    \n",
    "    # remove saccades and smooth pursuits with amplitude below 1.5\n",
    "    test_events_df = test_events_df.drop(test_events_df.loc[(test_events_df.amplitude<1.5)&(test_events_df.event=='sac')].index)\n",
    "    test_events_df.reset_index(inplace=True)\n",
    "    del test_events_df['index']\n",
    "    test_events_df = test_events_df.drop(test_events_df.loc[(test_events_df.amplitude<1.5)&(test_events_df.event=='smp')].index)\n",
    "    test_events_df.reset_index(inplace=True)\n",
    "    del test_events_df['index']\n",
    "    \n",
    "    # remove fixations with duration less than 100 ms\n",
    "    test_events_df = test_events_df.drop(test_events_df.loc[(test_events_df.duration<0.1)&(test_events_df.event=='fix')].index)\n",
    "    test_events_df.reset_index(inplace=True)\n",
    "    del test_events_df['index']\n",
    "    \n",
    "    # remove events with calculus error greater than 1.5\n",
    "    test_events_df = test_events_df.drop(test_events_df.loc[test_events_df.calculus_error>1.5].index)\n",
    "    test_events_df.reset_index(inplace=True)\n",
    "    del test_events_df['index']\n",
    "    \n",
    "    # if saccades are longer than the maximum possible duration of a saccade (0.613sec) - this is a check\n",
    "#     # (given that the max span of human vision is 180deg horizontal and 200deg vertical and max vel is 1000deg/s)\n",
    "#     max_sac_dur = 0.6\n",
    "#     test_events_df.loc[test_events_df[(test_events_df.event=='sac') & (test_events_df.duration > max_sac_dur)].index.tolist(),'event']='noise'\n",
    "\n",
    "        \n",
    "    #\n",
    "    # Step 6: Append to output event sequence\n",
    "    #\n",
    "    events_df['subID'] = subID\n",
    "    events_df['task'] = task\n",
    "    events_df['VL'] = vision_loss[subID]\n",
    "    events_df['HMD'] = DEVICE\n",
    "    events_df['eye'] = eye\n",
    "    events_df['freq'] = sample_rates[DEVICE]\n",
    "    # Want one combined csv of sequences, all events_df labelled by device, subID, VL, and task\n",
    "    sequences = pd.DataFrame({'HMD':events_df.HMD,\n",
    "                            'rate':events_df.freq,\n",
    "                            'eye':events_df.eye,\n",
    "                            'task':events_df.task,\n",
    "                            'subID':events_df.subID,\n",
    "                            'VL':events_df.VL,\n",
    "                            'event':events_df.event,\n",
    "                            'start_i':events_df.start_i,\n",
    "                            'end_i':events_df.end_i,\n",
    "                            'start_s':events_df.start_s,\n",
    "                            'end_s':events_df.end_s,\n",
    "                            'center_x':events_df.center_x,\n",
    "                            'center_y':events_df.center_y,\n",
    "                            'x0':events_df.x0,\n",
    "                            'y0':events_df.y0,\n",
    "                            'xn':events_df.xn,\n",
    "                            'yn':events_df.yn,\n",
    "                            'cov_x':events_df.cov_x,\n",
    "                            'cov_y':events_df.cov_y,\n",
    "                            'cov_xy':events_df.cov_xy,\n",
    "                            'amplitude':events_df.amplitude,\n",
    "                            'dispersion':events_df.dispersion,\n",
    "                            'duration':events_df.duration,\n",
    "                            'max_vel':events_df.max_vel,\n",
    "                            'avg_vel':events_df.avg_vel,\n",
    "                            'std_vel':events_df.std_vel,\n",
    "                            'max_Dn':events_df.max_Dn,\n",
    "                            'avg_Dn':events_df.avg_Dn,\n",
    "                            'max_iss':events_df.max_iss,\n",
    "                            'avg_iss':events_df.avg_iss,\n",
    "                            'calculus_error':events_df.calculus_error,\n",
    "                            'carpenter_error':events_df.carpenter_error,\n",
    "                            'P_nonfix':events_df.P_nonfix,\n",
    "                            'P_fix':events_df.P_fix,\n",
    "                            'P_ff':events_df.P_ff,\n",
    "                            'P_smp':events_df.P_smp,\n",
    "                            'P_sac':events_df.P_sac,\n",
    "                            'P_blink':events_df.P_blink})\n",
    "    test_seq = pd.DataFrame({'HMD':DEVICE,\n",
    "                            'rate':sample_rates[DEVICE],\n",
    "                            'eye':eye,\n",
    "                            'task':task,\n",
    "                            'subID':subID,\n",
    "                            'VL':vision_loss[subID],\n",
    "                            'event':test_events_df.event,\n",
    "                            'start_i':test_events_df.start_i,\n",
    "                            'end_i':test_events_df.end_i,\n",
    "                            'start_s':test_events_df.start_s,\n",
    "                            'end_s':test_events_df.end_s,\n",
    "                            'center_x':test_events_df.center_x,\n",
    "                            'center_y':test_events_df.center_y,\n",
    "                            'x0':test_events_df.x0,\n",
    "                            'y0':test_events_df.y0,\n",
    "                            'xn':test_events_df.xn,\n",
    "                            'yn':test_events_df.yn,\n",
    "                            'cov_x':test_events_df.cov_x,\n",
    "                            'cov_y':test_events_df.cov_y,\n",
    "                            'cov_xy':test_events_df.cov_xy,\n",
    "                            'amplitude':test_events_df.amplitude,\n",
    "                            'dispersion':test_events_df.dispersion,\n",
    "                            'duration':test_events_df.duration,\n",
    "                            'max_vel':test_events_df.max_vel,\n",
    "                            'avg_vel':test_events_df.avg_vel,\n",
    "                            'std_vel':test_events_df.std_vel,\n",
    "                            'max_Dn':test_events_df.max_Dn,\n",
    "                            'avg_Dn':test_events_df.avg_Dn,\n",
    "                            'max_iss':test_events_df.max_iss,\n",
    "                            'avg_iss':test_events_df.avg_iss,\n",
    "                            'calculus_error':test_events_df.calculus_error,\n",
    "                            'carpenter_error':test_events_df.carpenter_error,\n",
    "                            'P_nonfix':test_events_df.P_nonfix,\n",
    "                            'P_fix':test_events_df.P_fix,\n",
    "                            'P_ff':test_events_df.P_ff,\n",
    "                            'P_smp':test_events_df.P_smp,\n",
    "                            'P_sac':test_events_df.P_sac,\n",
    "                            'P_blink':test_events_df.P_blink,\n",
    "                            'has_blink':test_events_df.has_blink})\n",
    "    test_seq_51 = pd.DataFrame({'HMD':DEVICE,\n",
    "                            'rate':sample_rates[DEVICE],\n",
    "                            'eye':eye,\n",
    "                            'task':task,\n",
    "                            'subID':subID,\n",
    "                            'VL':vision_loss[subID],\n",
    "                            'event':test_events_df_51.event,\n",
    "                            'start_i':test_events_df_51.start_i,\n",
    "                            'end_i':test_events_df_51.end_i,\n",
    "                            'start_s':test_events_df_51.start_s,\n",
    "                            'end_s':test_events_df_51.end_s,\n",
    "                            'center_x':test_events_df_51.center_x,\n",
    "                            'center_y':test_events_df_51.center_y,\n",
    "                            'x0':test_events_df_51.x0,\n",
    "                            'y0':test_events_df_51.y0,\n",
    "                            'xn':test_events_df_51.xn,\n",
    "                            'yn':test_events_df_51.yn,\n",
    "                            'cov_x':test_events_df_51.cov_x,\n",
    "                            'cov_y':test_events_df_51.cov_y,\n",
    "                            'cov_xy':test_events_df_51.cov_xy,\n",
    "                            'amplitude':test_events_df_51.amplitude,\n",
    "                            'dispersion':test_events_df_51.dispersion,\n",
    "                            'duration':test_events_df_51.duration,\n",
    "                            'max_vel':test_events_df_51.max_vel,\n",
    "                            'avg_vel':test_events_df_51.avg_vel,\n",
    "                            'std_vel':test_events_df_51.std_vel,\n",
    "                            'max_Dn':test_events_df_51.max_Dn,\n",
    "                            'avg_Dn':test_events_df_51.avg_Dn,\n",
    "                            'max_iss':test_events_df_51.max_iss,\n",
    "                            'avg_iss':test_events_df_51.avg_iss,\n",
    "                            'calculus_error':test_events_df_51.calculus_error,\n",
    "                            'carpenter_error':test_events_df_51.carpenter_error,\n",
    "                            'P_nonfix':test_events_df_51.P_nonfix,\n",
    "                            'P_fix':test_events_df_51.P_fix,\n",
    "                            'P_ff':test_events_df_51.P_ff,\n",
    "                            'P_smp':test_events_df_51.P_smp,\n",
    "                            'P_sac':test_events_df_51.P_sac,\n",
    "                            'P_blink':test_events_df_51.P_blink,\n",
    "                            'has_blink':test_events_df_51.has_blink})\n",
    "    test_seq_52 = pd.DataFrame({'HMD':DEVICE,\n",
    "                            'rate':sample_rates[DEVICE],\n",
    "                            'eye':eye,\n",
    "                            'task':task,\n",
    "                            'subID':subID,\n",
    "                            'VL':vision_loss[subID],\n",
    "                            'event':test_events_df_52.event,\n",
    "                            'start_i':test_events_df_52.start_i,\n",
    "                            'end_i':test_events_df_52.end_i,\n",
    "                            'start_s':test_events_df_52.start_s,\n",
    "                            'end_s':test_events_df_52.end_s,\n",
    "                            'center_x':test_events_df_52.center_x,\n",
    "                            'center_y':test_events_df_52.center_y,\n",
    "                            'x0':test_events_df_52.x0,\n",
    "                            'y0':test_events_df_52.y0,\n",
    "                            'xn':test_events_df_52.xn,\n",
    "                            'yn':test_events_df_52.yn,\n",
    "                            'cov_x':test_events_df_52.cov_x,\n",
    "                            'cov_y':test_events_df_52.cov_y,\n",
    "                            'cov_xy':test_events_df_52.cov_xy,\n",
    "                            'amplitude':test_events_df_52.amplitude,\n",
    "                            'dispersion':test_events_df_52.dispersion,\n",
    "                            'duration':test_events_df_52.duration,\n",
    "                            'max_vel':test_events_df_52.max_vel,\n",
    "                            'avg_vel':test_events_df_52.avg_vel,\n",
    "                            'std_vel':test_events_df_52.std_vel,\n",
    "                            'max_Dn':test_events_df_52.max_Dn,\n",
    "                            'avg_Dn':test_events_df_52.avg_Dn,\n",
    "                            'max_iss':test_events_df_52.max_iss,\n",
    "                            'avg_iss':test_events_df_52.avg_iss,\n",
    "                            'calculus_error':test_events_df_52.calculus_error,\n",
    "                            'carpenter_error':test_events_df_52.carpenter_error,\n",
    "                            'P_nonfix':test_events_df_52.P_nonfix,\n",
    "                            'P_fix':test_events_df_52.P_fix,\n",
    "                            'P_ff':test_events_df_52.P_ff,\n",
    "                            'P_smp':test_events_df_52.P_smp,\n",
    "                            'P_sac':test_events_df_52.P_sac,\n",
    "                            'P_blink':test_events_df_52.P_blink,\n",
    "                            'has_blink':test_events_df_52.has_blink})\n",
    "    \n",
    "    #\n",
    "    # save results\n",
    "    #\n",
    "    sequences.to_csv(f'{out_path}50/{ID}_50.csv')\n",
    "    test_seq_51.to_csv(f'{out_path}51/{ID}_51.csv')\n",
    "    test_seq_52.to_csv(f'{out_path}52/{ID}_52.csv')\n",
    "    test_seq.to_csv(f'{out_path}{ID}.csv')\n",
    "    print(f'[{NUM}] {ID} success!')\n",
    "    NUM += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce276588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results - 5.0\n",
    "in_path = '/data/Isabella/thesis_spring2022/event_detect_out_final/50/'\n",
    "i = 0\n",
    "for fname in os.listdir(in_path):\n",
    "    if 'all_sequences' in fname or 'PI' in fname:\n",
    "        continue\n",
    "    if i == 0:\n",
    "        seq = pd.read_csv(in_path+fname)\n",
    "        i += 1\n",
    "    else:\n",
    "        seq = pd.concat([seq, pd.read_csv(in_path+fname)])\n",
    "seq.to_csv(in_path+'all_sequences_50_varjo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae49a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results - 5.1\n",
    "in_path = '/data/Isabella/thesis_spring2022/event_detect_out_final/51/'\n",
    "i = 0\n",
    "for fname in os.listdir(in_path):\n",
    "    if 'all_sequences' in fname or 'PI' in fname:\n",
    "        continue\n",
    "    if i == 0:\n",
    "        seq = pd.read_csv(in_path+fname)\n",
    "        i += 1\n",
    "    else:\n",
    "        seq = pd.concat([seq, pd.read_csv(in_path+fname)])\n",
    "seq.to_csv(in_path+'all_sequences_51_varjo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54dbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results - 5.2\n",
    "in_path = '/data/Isabella/thesis_spring2022/event_detect_out_final/52/'\n",
    "i = 0\n",
    "for fname in os.listdir(in_path):\n",
    "    if 'all_sequences' in fname or 'PI' in fname:\n",
    "        continue\n",
    "    if i == 0:\n",
    "        seq = pd.read_csv(in_path+fname)\n",
    "        i += 1\n",
    "    else:\n",
    "        seq = pd.concat([seq, pd.read_csv(in_path+fname)])\n",
    "seq.to_csv(in_path+'all_sequences_52_varjo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7c93c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge results - final\n",
    "in_path = '/data/Isabella/thesis_spring2022/event_detect_out_final/'\n",
    "i = 0\n",
    "for fname in os.listdir(in_path):\n",
    "    if fname == 'archive' or 'all_sequences' in fname or 'PI' in fname:\n",
    "        continue\n",
    "    if '50' not in fname and '51' not in fname and '52' not in fname:\n",
    "        if i == 0:\n",
    "            seq = pd.read_csv(in_path+fname)\n",
    "            i += 1\n",
    "        else:\n",
    "            seq = pd.concat([seq, pd.read_csv(in_path+fname)])\n",
    "seq.to_csv(in_path+'all_sequences_varjo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d9730",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
